{"cells":[{"cell_type":"code","execution_count":null,"id":"B-sZU_XAWViM","metadata":{"id":"B-sZU_XAWViM"},"outputs":[],"source":["# Load the Drive helper and mount\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# ======================= #\n","#     Strat of Classification Codes    #\n","# ======================= #\n","\n","# ## **Step 1: Automated Package Installation**\n","\n","# - **Input:**\n","#   - List of required packages for the analysis.\n","\n","# - **Process:**\n","#   - **1.1** Import necessary modules: `sys`, `subprocess`.\n","#   - **1.2** Define the `install(package)` function to install missing packages using `pip`.\n","#   - **1.3** Create a list of required packages:\n","#     - `'skrebate'`, `'rulefit'`, `'scikit-learn'`, `'pandas'`, `'numpy'`, `'matplotlib'`, `'seaborn'`, `'xgboost'`, `'lightgbm'`, `'catboost'`.\n","#   - **1.4** Iterate through the list of required packages:\n","#     - Try to import each package using `__import__(package)`.\n","#     - If an `ImportError` occurs, print a message and call `install(package)` to install it.\n","\n","# - **Output:**\n","#   - All required packages are installed and available for use in the environment.\n","\n","# ---\n","\n","# ## **Step 2: Importing Modules**\n","\n","# - **Input:**\n","#   - None (standard and third-party libraries).\n","\n","# - **Process:**\n","#   - **2.1** Import essential libraries for system operations and utility functions:\n","#     - `os`, `shutil`, `inspect`, `datetime`.\n","#   - **2.2** Import data manipulation and numerical computation libraries:\n","#     - `pandas` as `pd`, `numpy` as `np`.\n","#   - **2.3** Import visualization libraries:\n","#     - `matplotlib.pyplot` as `plt`, `seaborn` as `sns`.\n","#   - **2.4** Import scikit-learn modules for data preprocessing:\n","#     - Imputers: `SimpleImputer`, `KNNImputer`, `IterativeImputer` (after enabling experimental features).\n","#     - Scalers: `MinMaxScaler`, `StandardScaler`, `RobustScaler`, `Normalizer`, `MaxAbsScaler`.\n","#   - **2.5** Import scikit-learn modules for model selection and evaluation:\n","#     - `train_test_split`, `StratifiedKFold`, `GridSearchCV`, `RandomizedSearchCV`.\n","#     - Metrics: `accuracy_score`, `precision_score`, `recall_score`, `f1_score`, `confusion_matrix`.\n","#   - **2.6** Import feature selection methods:\n","#     - `SelectKBest`, `chi2`, `f_classif`, `mutual_info_classif`.\n","#   - **2.7** Import machine learning classifiers:\n","#     - Linear models: `LogisticRegression`, `Lasso`.\n","#     - Tree-based models: `DecisionTreeClassifier`, `RandomForestClassifier`.\n","#     - Ensemble methods: `StackingClassifier`.\n","#     - Support Vector Machine: `SVC`.\n","#     - Neural Networks: `MLPClassifier`.\n","#     - Discriminant Analysis: `LinearDiscriminantAnalysis`.\n","#     - Nearest Neighbors: `KNeighborsClassifier`.\n","#     - Naive Bayes: `GaussianNB`.\n","#     - Gradient Boosting models: `XGBClassifier`, `LGBMClassifier`, `CatBoostClassifier`.\n","#     - Rule-based model: `RuleFit`.\n","#   - **2.8** Enable experimental features if necessary (e.g., `enable_iterative_imputer`).\n","\n","# - **Output:**\n","#   - All necessary libraries are imported and ready for use in the script.\n","\n","# ---\n","\n","# ## **Step 3: Parameters Configuration**\n","\n","# - **Input:**\n","#   - None (parameters are defined within the script).\n","\n","# - **Process:**\n","#   - **3.1 General Settings:**\n","#     - **3.1.1** Set random seed for reproducibility:\n","#       - `RANDOM_SEED = 11`.\n","#     - **3.1.2** Define the number of top features to select:\n","#       - `NOF = 5`.\n","#     - **3.1.3** Set the number of folds for cross-validation:\n","#       - `N_FOLDS = 5`.\n","#     - **3.1.4** Define test size for train-test split:\n","#       - `TEST_SIZE = 0.15` (i.e., 15% of data for testing).\n","#   - **3.2 Classes Selection and Mapping:**\n","#     - **3.2.1** Select specific classes to include:\n","#       - `SELECTED_CLASSES = [0, 1, 2, 3, 4, 5]` (adjust as needed).\n","#     - **3.2.2** Define class mapping to consolidate classes:\n","#       - Map classes `0`, `1`, `2`, `3` to `0` (negative class).\n","#       - Map classes `4`, `5` to `1` (positive class).\n","#       - `CLASS_MAPPING = {0: 0, 1: 0, 2: 0, 3: 0, 4: 1, 5: 1}`.\n","#   - **3.3 Class Selection Percentages:**\n","#     - **3.3.1** Define the percentage of patients to select from each class before mapping:\n","#       - `CLASS_SELECTION_PERCENT = {0: 1, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1}` (100% selection for all classes).\n","#   - **3.4 Data Paths:**\n","#     - **3.4.1** Define the base directory for saving results:\n","#       - `BASE_RESULTS_DIRECTORY = r'Add the path of the folder that you would like to store the results'`.\n","#     - **3.4.2** Specify the file path for the input data:\n","#       - `FILE_PATH = r'Add a excel file with .xlsx format'`.\n","#     - **3.4.3** Set the sheet name for Excel files:\n","#       - `SHEET_NAME = 0`.\n","#   - **3.5 Imputation Strategy:**\n","#     - **3.5.1** Specify the strategy for handling missing values:\n","#       - `IMPUTATION_STRATEGY = 'SimpleImputer_mean'`.\n","#   - **3.6 Scaling Method:**\n","#     - **3.6.1** Specify the method for scaling features:\n","#       - `SCALING_METHOD = 'MinMaxScaler'`.\n","#   - **3.7 Feature Selectors:**\n","#     - **3.7.1** Define feature selection methods and their parameters in `INVOLVED_FEATURE_SELECTORS`:\n","#       - Examples include:\n","#         - 'Chi-Square Test (CST)'\n","#         - 'Correlation Coefficient (CC)'\n","#         - 'Mutual Information (MI)'\n","#         - 'Variance Threshold (VT)'\n","#         - 'ANOVA F-test (AFT)'\n","#         - 'Information Gain (IG)'\n","#         - 'Univariate Feature Selection (UFS)'\n","#         - 'Fisher Score (FS)'\n","#         - 'LASSO'\n","#   - **3.8 Classifiers:**\n","#     - **3.8.1** Define classifiers, their initial parameters, and hyperparameter grids in `INVOLVED_CLASSIFIERS`:\n","#       - Classifiers include:\n","#         - **Decision Tree Classification (DTC)**\n","#         - **Logistic Regression (LR)**\n","#         - **Linear Discriminant Analysis (LDA)**\n","#         - **Naive Bayes Classifier (NBC)**\n","#         - **K-Nearest Neighbors (KNN)**\n","#         - **Random Forest Classifier (RFC)**\n","#         - **Support Vector Machine (SVM)**\n","#         - **XGBoost Classifier**\n","#         - **LightGBM Classifier**\n","#         - **CatBoost Classifier**\n","#         - **Stacking Classifier**\n","#         - **MLP Classifier (MLP)**\n","#         - **RuleFit Classifier (RUC)**\n","#       - Each classifier has:\n","#         - A model class.\n","#         - Initial parameters (`params`).\n","#         - A hyperparameter grid (`param_grid`) for tuning.\n","#   - **3.9 Grid Search Configuration:**\n","#     - **3.9.1** Set the grid search mode:\n","#       - `GRID_SEARCH_MODE = 'randomized'`.\n","#     - **3.9.2** Define the number of iterations for randomized search:\n","#       - `GRID_SEARCH_ITER = 5`.\n","\n","# - **Output:**\n","#   - All configuration parameters are set and ready for use in the analysis.\n","\n","# ---\n","\n","# ## **Step 4: Data Reading, Shuffling, Class Selection, and Mapping**\n","\n","# - **Input:**\n","#   - The data file specified by `FILE_PATH` (supports `.xlsx`, `.xls`, `.csv` formats).\n","\n","# - **Process:**\n","#   - **4.1 Data Reading:**\n","#     - **4.1.1** Read the data file using `pandas` based on the file extension.\n","#       - If the file is Excel (`.xlsx`, `.xls`), use `pd.read_excel(FILE_PATH, sheet_name=SHEET_NAME)`.\n","#       - If the file is CSV (`.csv`), use `pd.read_csv(FILE_PATH)`.\n","#     - **4.1.2** Extract components from the data:\n","#       - `Patient_ID`: The first column.\n","#       - `Data`: All columns from the second to the second-to-last.\n","#       - `Outcome`: The last column.\n","#   - **4.2 Diagnostics Before Mapping:**\n","#     - **4.2.1** Print unique values in `Outcome` before filtering and mapping.\n","#     - **4.2.2** Check and print the number of missing values in `Outcome`.\n","#   - **4.3 Data Filtering and Shuffling:**\n","#     - **4.3.1** Filter the dataset to include only `SELECTED_CLASSES`.\n","#       - Create a mask and apply it to the data.\n","#     - **4.3.2** Shuffle the filtered data if necessary to ensure randomness.\n","#     - **4.3.3** Update `Patient_ID`, `Data`, and `Outcome` after filtering and shuffling.\n","#   - **4.4 Class Selection by Percentage:**\n","#     - **4.4.1** Define a function `select_percentage_per_class` to select a specified percentage of patients from each class.\n","#     - **4.4.2** Apply the function to select data as per `CLASS_SELECTION_PERCENT`.\n","#     - **4.4.3** Update `Patient_ID`, `Data`, and `Outcome` after selection.\n","#     - **4.4.4** Verify the selection by printing class distribution after selection.\n","#   - **4.5 Class Mapping:**\n","#     - **4.5.1** Map original classes to new classes using `CLASS_MAPPING`.\n","#       - Use `Outcome_mapped = Outcome.map(CLASS_MAPPING)`.\n","#     - **4.5.2** Ensure no missing values after mapping.\n","#       - If missing values are found (unmapped classes), raise a `ValueError`.\n","#     - **4.5.3** Update `Outcome` with the mapped classes.\n","#   - **4.6 Feature Separation:**\n","#     - **4.6.1** Identify numeric columns using `Data.select_dtypes(include=[np.number])`.\n","#     - **4.6.2** Identify non-numeric columns using `Data.select_dtypes(exclude=[np.number])`.\n","#     - **4.6.3** Separate numeric and non-numeric data.\n","\n","# - **Output:**\n","#   - Prepared `Patient_ID`, `Data`, and `Outcome` variables for further processing.\n","\n","# ---\n","\n","# ## **Step 5: Preprocessing**\n","\n","# - **Input:**\n","#   - `Data`, `Outcome`.\n","\n","# - **Process:**\n","#   - **5.1 Stratified Splitting:**\n","#     - **5.1.1** Perform a stratified train-test split using `train_test_split` to maintain class distribution.\n","#       - Split data into:\n","#         - `X_train_num`, `X_test_num`: Numeric features.\n","#         - `y_train`, `y_test`: Target variable.\n","#         - `Patient_ID_train`, `Patient_ID_test`: Patient identifiers.\n","#       - Parameters:\n","#         - `test_size=TEST_SIZE`\n","#         - `random_state=RANDOM_SEED`\n","#         - `stratify=Outcome`\n","#   - **5.2 Imputation:**\n","#     - **5.2.1** Initialize the imputer based on `IMPUTATION_STRATEGY`.\n","#       - For `'SimpleImputer_mean'`, use `SimpleImputer(strategy='mean')`.\n","#     - **5.2.2** Fit the imputer on `X_train_num` and transform both training and testing data:\n","#       - `X_train_num_imputed = imputer.fit_transform(X_train_num)`\n","#       - `X_test_num_imputed = imputer.transform(X_test_num)`\n","#   - **5.3 Scaling:**\n","#     - **5.3.1** Initialize the scaler based on `SCALING_METHOD`.\n","#       - For `'MinMaxScaler'`, use `MinMaxScaler()`.\n","#     - **5.3.2** Fit the scaler on the imputed training data and transform both training and testing data:\n","#       - `X_train_scaled = scaler.fit_transform(X_train_num_imputed)`\n","#       - `X_test_scaled = scaler.transform(X_test_num_imputed)`\n","\n","# - **Output:**\n","#   - `X_train_scaled` and `X_test_scaled`: Preprocessed feature matrices ready for feature selection and modeling.\n","\n","# ---\n","\n","# ## **Step 6: Feature Selection**\n","\n","# - **Input:**\n","#   - `X_train_scaled`, `y_train`, feature selectors, `NOF`.\n","\n","# - **Process:**\n","#   - **6.1 Define Feature Selector Functions:**\n","#     - Implement functions for each feature selection method:\n","#       - **6.1.1** `apply_correlation_coefficient`:\n","#         - Calculate the absolute correlation between each feature and the target.\n","#         - Select the top `NOF` features with the highest correlation.\n","#       - **6.1.2** `apply_chi_square`:\n","#         - Use `SelectKBest` with `score_func=chi2` to select top `NOF` features.\n","#       - **6.1.3** `apply_mutual_information`:\n","#         - Use `SelectKBest` with `score_func=mutual_info_classif` to select top `NOF` features.\n","#       - **6.1.4** `apply_variance_threshold`:\n","#         - Calculate variance of each feature and select top `NOF` features with highest variance.\n","#       - **6.1.5** `apply_anova_f_test`:\n","#         - Use `SelectKBest` with `score_func=f_classif` to select top `NOF` features.\n","#       - **6.1.6** `apply_information_gain`:\n","#         - Equivalent to mutual information; use `apply_mutual_information`.\n","#       - **6.1.7** `apply_univariate_feature_selection`:\n","#         - Use `SelectKBest` with a specified `score_func` to select top `NOF` features.\n","#       - **6.1.8** `apply_fisher_score`:\n","#         - Manually compute Fisher Scores and select top `NOF` features.\n","#       - **6.1.9** `apply_lasso`:\n","#         - Use `Lasso` regression to select features with non-zero coefficients.\n","#         - Adjust for cases where fewer than `NOF` features are selected.\n","#   - **6.2 Map Functions:**\n","#     - **6.2.1** Create a dictionary `feature_selector_functions` mapping function names to the actual functions.\n","\n","# - **Output:**\n","#   - Feature selector functions are ready to be applied.\n","\n","# ---\n","\n","# ## **Step 7: Applying Feature Selection and Classifiers**\n","\n","# - **Input:**\n","#   - Preprocessed data (`X_train_scaled`, `X_test_scaled`), `y_train`, `y_test`, `Patient_ID_train`, `Patient_ID_test`, feature selectors, classifiers, grid search configuration.\n","\n","# - **Process:**\n","#   - **7.1 Initialize Storage Structures:**\n","#     - **7.1.1** Initialize lists and dictionaries to store results, selected features, confusion matrices, and best hyperparameters.\n","#     - **7.1.2** Create a timestamped `results_directory` to store outputs.\n","#     - **7.1.3** Create subdirectories:\n","#       - `Predicted_Outcome/Fivefold Cross Validation`\n","#       - `Predicted_Outcome/External Testing`\n","#       - `Tuning_Hyperparameters`\n","#   - **7.2 Initialize Stratified K-Fold:**\n","#     - **7.2.1** Set up `StratifiedKFold` with `n_splits=N_FOLDS`, `shuffle=True`, `random_state=RANDOM_SEED`.\n","#   - **7.3 Iterate Over Feature Selectors:**\n","#     - For each feature selector in `feature_selectors`:\n","#       - **7.3.1 Apply Feature Selection:**\n","#         - **7.3.1.1** Retrieve the function and parameters from `INVOLVED_FEATURE_SELECTORS`.\n","#         - **7.3.1.2** Apply the feature selection function to `X_train_scaled` and `y_train`.\n","#         - **7.3.1.3** Obtain selected feature indices and names.\n","#         - **7.3.1.4** Apply the same feature selection to `X_test_scaled`.\n","#         - **7.3.1.5** Store selected features for later reference.\n","#       - **7.3.2 Iterate Over Classifiers:**\n","#         - For each classifier in `classifiers`:\n","#           - **7.3.2.1 Hyperparameter Tuning:**\n","#             - **7.3.2.1.1** Retrieve the model class, initial parameters, and hyperparameter grid from `INVOLVED_CLASSIFIERS`.\n","#             - **7.3.2.1.2** Initialize the classifier with initial parameters.\n","#             - **7.3.2.1.3** Use `RandomizedSearchCV` or `GridSearchCV` for hyperparameter tuning:\n","#               - If `GRID_SEARCH_MODE == 'randomized'`, use `RandomizedSearchCV` with `n_iter=GRID_SEARCH_ITER`.\n","#               - Handle cases where `param_grid` is a list (switch to `GridSearchCV`).\n","#             - **7.3.2.1.4** Fit the search object on `X_train_selected` and `y_train`.\n","#             - **7.3.2.1.5** Retrieve and store the best hyperparameters.\n","#           - **7.3.2.2 K-Fold Cross-Validation:**\n","#             - **7.3.2.2.1** Initialize metrics storage for each fold.\n","#             - **7.3.2.2.2** For each fold in `StratifiedKFold`:\n","#               - **7.3.2.2.2.1** Split `X_train_selected` and `y_train` into training and validation sets.\n","#               - **7.3.2.2.2.2** Instantiate a new classifier with the best hyperparameters.\n","#               - **7.3.2.2.2.3** Train the classifier on the training fold.\n","#               - **7.3.2.2.2.4** Predict on the validation fold.\n","#               - **7.3.2.2.2.5** Compute evaluation metrics: Accuracy, Precision, Recall, F1-Score.\n","#               - **7.3.2.2.2.6** Store predictions, true labels, patient IDs, and fold numbers.\n","#               - **7.3.2.2.2.7** Predict on the external test set.\n","#               - **7.3.2.2.2.8** Compute test metrics and store predictions.\n","#               - **7.3.2.2.2.9** Compute and store confusion matrices for both validation and test sets.\n","#             - **7.3.2.2.3** Aggregate metrics across folds and compute means and standard deviations.\n","#             - **7.3.2.2.4** Save cross-validation and test predictions to CSV files in their respective directories.\n","#           - **7.3.2.3 Save Best Hyperparameters:**\n","#             - **7.3.2.3.1** Save the best hyperparameters for the classifier to a CSV file in `Tuning_Hyperparameters`.\n","#       - **7.3.3 Save Selected Features:**\n","#         - **7.3.3.1** Save the list of selected features for the feature selector to `selected_features.csv`.\n","\n","# - **Output:**\n","#   - Results including evaluation metrics, selected features, confusion matrices, and best hyperparameters are collected and ready for saving.\n","\n","# ---\n","\n","# ## **Step 8: Saving and Aggregating Results**\n","\n","# - **Input:**\n","#   - Collected results from Step 7.\n","\n","# - **Process:**\n","#   - **8.1 Aggregate Metrics:**\n","#     - **8.1.1** Concatenate results from all classifiers and feature selectors into a single DataFrame (`results_df`).\n","#     - **8.1.2** Compute average metrics (mean and standard deviation) grouped by `Feature Selector` and `Classifier`:\n","#       - Metrics include:\n","#         - Validation and Test Accuracy\n","#         - Precision\n","#         - Recall\n","#         - F1-Score\n","#   - **8.2 Save Metrics:**\n","#     - **8.2.1** Save detailed evaluation metrics to `evaluation_metrics.csv` in `results_directory`.\n","#     - **8.2.2** Save average metrics to `average_metrics.csv`.\n","#     - **8.2.3** Extract and save standard deviations to `STD_metrics.csv`.\n","#   - **8.3 Save Confusion Matrices:**\n","#     - **8.3.1** Save all confusion matrices to `confusion_matrices.csv`.\n","#   - **8.4 Copy Original Data File:**\n","#     - **8.4.1** Copy the original data file to the `results_directory` for reference.\n","#   - **8.5 Save Workflow and Code:**\n","#     - **8.5.1** Save the workflow description as `Workflow_Ver18.txt`.\n","#     - **8.5.2** Save the script code as `Code_Ver18.py`:\n","#       - Use `inspect.getsource` to retrieve the code.\n","#       - Handle exceptions if `inspect` cannot retrieve the code.\n","#   - **8.6 Save Best Hyperparameters:**\n","#     - **8.6.1** Combine best hyperparameters from all classifiers and feature selectors.\n","#     - **8.6.2** Save to `best_parameters.csv` in `Tuning_Hyperparameters`.\n","\n","# - **Output:**\n","#   - All results are saved in the organized `results_directory`.\n","\n","# ---\n","\n","# ## **Step 9: Final Output Messages**\n","\n","# - **Input:**\n","#   - None.\n","\n","# - **Process:**\n","#   - **9.1** Print messages summarizing the completion of the script and locations of saved files:\n","#     - Results files (evaluation metrics, average metrics, standard deviations).\n","#     - Selected features.\n","#     - Confusion matrices.\n","#     - Predictions from cross-validation and testing.\n","#     - Best hyperparameters.\n","#     - Original data file.\n","#     - Workflow description and code.\n","#   - **9.2** If no results are available, inform the user to check data and parameters.\n","\n","# - **Output:**\n","#   - User is informed about the successful completion and where to find all outputs.\n","\n","# # ======================= #\n","# #    End of Classification Codes      #\n","# # ======================= #\n"],"metadata":{"id":"vz3SUo82nUws"},"id":"vz3SUo82nUws","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ======================= #\n","#      Start of Classification Codes         #\n","# ======================= #\n","\n","# ----------------------- #\n","# 1. Automated Package Installation\n","# ----------------------- #\n","import sys\n","import subprocess\n","\n","def install(package):\n","    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n","\n","# List of required packages\n","required_packages = [\n","    'skrebate',\n","    'rulefit',\n","    'scikit-learn',  # Ensure scikit-learn is up-to-date\n","    'pandas',\n","    'numpy',\n","    'matplotlib',\n","    'seaborn',\n","    'xgboost',       # XGBoost\n","    'lightgbm',      # LightGBM\n","    'catboost',      # CatBoost\n","]\n","\n","# Install missing packages\n","for package in required_packages:\n","    try:\n","        __import__(package)\n","    except ImportError:\n","        print(f\"Installing package: {package}\")\n","        install(package)\n","\n","# ----------------------- #\n","# 2. Importing Modules\n","# ----------------------- #\n","import os\n","import shutil\n","import inspect  # For getting the source code\n","import pandas as pd\n","import numpy as np\n","from datetime import datetime\n","import matplotlib.pyplot as plt  # For plotting\n","import seaborn as sns  # For plotting\n","from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, RandomizedSearchCV\n","from xgboost import XGBClassifier  # Importing XGBoost\n","from lightgbm import LGBMClassifier  # Importing LightGBM\n","from catboost import CatBoostClassifier  # Importing CatBoost\n","from sklearn.ensemble import RandomForestClassifier, StackingClassifier  # Random Forest and Stacking\n","from sklearn.svm import SVC  # Support Vector Machine\n","from sklearn.neural_network import MLPClassifier  # Importing MLPClassifier\n","\n","# Enable IterativeImputer before importing it\n","from sklearn.experimental import enable_iterative_imputer  # Enable the experimental feature\n","from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer  # Now import IterativeImputer\n","from sklearn.preprocessing import (\n","    MinMaxScaler, StandardScaler, RobustScaler, Normalizer, MaxAbsScaler\n",")\n","from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif\n","from sklearn.linear_model import LogisticRegression, Lasso\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from sklearn.tree import DecisionTreeClassifier\n","from rulefit import RuleFit\n","from sklearn.metrics import (\n","    accuracy_score,\n","    precision_score,\n","    recall_score,\n","    f1_score,\n","    confusion_matrix,\n",")\n","\n","# ----------------------- #\n","# 3. Parameters Configuration\n","# ----------------------- #\n","\n","### PARAMETERS ###\n","\n","# --------------------- #\n","# General Settings\n","# --------------------- #\n","# Random seed for reproducibility\n","RANDOM_SEED = 11\n","\n","# Number of top features to select\n","NOF = 5\n","\n","# Number of folds for cross-validation\n","N_FOLDS = 5\n","\n","# Test size for train-test split\n","TEST_SIZE = 0.15   # TRAIN Size=1-TEST_SIZE\n","\n","# Classes Selection and Mapping\n","# Select specific classes to include\n","SELECTED_CLASSES = [0, 1, 2, 3, 4, 5]  # Adjust as needed\n","\n","# Define class mapping: map class 3 to 0, and classes 4 & 5 to 1\n","CLASS_MAPPING = {\n","    0: 0,\n","    1: 0,\n","    2: 0,\n","    3: 0,  # Example: You might want to adjust based on your specific needs\n","    4: 1,\n","    5: 1\n","}\n","print(\"Selected Classes:\", SELECTED_CLASSES)\n","print(\"CLASS_MAPPING:\", CLASS_MAPPING)\n","\n","# --------------------- #\n","# Class Selection Percentages\n","# --------------------- #\n","# Define the percentage of patients to select from each class before mapping\n","# Values should be between 0 and 1\n","CLASS_SELECTION_PERCENT = {\n","    0: 1,    # Select 100% of patients from class 0\n","    1: 1,    # Select 100% of patients from class 1\n","    2: 1,    # Select 100% of patients from class 2\n","    3: 1, # Select 100% of patients from class 3\n","    4: 1, # Select 100% of patients from class 4\n","    5: 1  # Select 100% of patients from class 5\n","}\n","\n","print(\"Class Selection Percentages:\", CLASS_SELECTION_PERCENT)\n","\n","# --------------------- #\n","# Data Paths\n","# --------------------- #\n","# Base directory for saving all results\n","BASE_RESULTS_DIRECTORY = r'Add the path of the folder that you would like to store the results'#  Saving path directory\n","\n","# File path for input data\n","FILE_PATH = r'Add a excel file with .xlsx formt'# The first column is patinet ID and the last column is true outcomes\n","\n","# Sheet name (if applicable)\n","SHEET_NAME = 0  # Set to the sheet name or index if reading from an Excel file\n","\n","# --------------------- #\n","# Imputation Strategy\n","# --------------------- #\n","# Define the imputation strategy for handling missing values.\n","# Options:\n","# - 'SimpleImputer_mean'\n","# - 'SimpleImputer_median'\n","# - 'SimpleImputer_most_frequent'\n","# - 'SimpleImputer_constant'\n","# - 'KNNImputer'\n","# - 'IterativeImputer'\n","IMPUTATION_STRATEGY = 'SimpleImputer_mean'  # Adjust as needed\n","\n","# --------------------- #\n","# Scaling Method\n","# --------------------- #\n","# Define the scaling method for preprocessing.\n","# Options:\n","# - 'MinMaxScaler'\n","# - 'StandardScaler'\n","# - 'RobustScaler'\n","# - 'Normalizer'\n","# - 'MaxAbsScaler'\n","# - None (No scaling)\n","SCALING_METHOD = 'MinMaxScaler'  # Adjust as needed\n","\n","# --------------------- #\n","# Feature Selectors\n","# --------------------- #\n","# Define all feature selectors to be included.\n","# Each key is a descriptive name, and the value contains the function name and parameters.\n","INVOLVED_FEATURE_SELECTORS = {\n","    'Chi-Square Test (CST)': {\n","        'function': 'apply_chi_square',\n","        'params': {'score_func': chi2},\n","    },\n","    'Correlation Coefficient (CC)': {\n","        'function': 'apply_correlation_coefficient',\n","        'params': {},\n","    },\n","    'Mutual Information (MI)': {\n","        'function': 'apply_mutual_information',\n","        'params': {},\n","    },\n","    'Variance Threshold (VT)': {\n","        'function': 'apply_variance_threshold',\n","        'params': {},\n","    },\n","    'ANOVA F-test (AFT)': {\n","        'function': 'apply_anova_f_test',\n","        'params': {},\n","    },\n","    'Information Gain (IG)': {\n","        'function': 'apply_information_gain',\n","        'params': {},\n","    },\n","    'Univariate Feature Selection (UFS)': {\n","        'function': 'apply_univariate_feature_selection',\n","        'params': {'score_func': mutual_info_classif},  # Example using MI\n","    },\n","    'Fisher Score (FS)': {\n","        'function': 'apply_fisher_score',\n","        'params': {},\n","    },\n","    'LASSO': {\n","        'function': 'apply_lasso',\n","        'params': {'alpha': 0.01},  # Adjust alpha as needed\n","    },\n","}\n","\n","# --------------------- #\n","# Classifiers\n","# --------------------- #\n","# Define all classifiers to be involved.\n","# Each key is a descriptive name, and the value contains the model class, initial parameters, and hyperparameter grid.\n","INVOLVED_CLASSIFIERS = {\n","    'Decision Tree Classification (DTC)': {\n","        'model': DecisionTreeClassifier,\n","        'params': {'random_state': RANDOM_SEED},\n","        'param_grid': {\n","            'max_depth': [None, 10, 20, 30],\n","            'min_samples_split': [2, 5, 10],\n","            'min_samples_leaf': [1, 2, 4],\n","            'criterion': ['gini', 'entropy']\n","        }\n","    },\n","    'Logistic Regression (LR)': {\n","        'model': LogisticRegression,\n","        'params': {'max_iter': 1000, 'random_state': RANDOM_SEED},\n","        'param_grid': {\n","            'C': [0.01, 0.1, 1, 10, 100],\n","            'penalty': ['l2'],\n","            'solver': ['lbfgs', 'saga']\n","        }\n","    },\n","    'Linear Discriminant Analysis (LDA)': {\n","        'model': LinearDiscriminantAnalysis,\n","        'params': {},\n","        'param_grid': [\n","            {'solver': ['svd']},\n","            {'solver': ['lsqr', 'eigen'], 'shrinkage': [None, 'auto']}\n","        ]\n","    },\n","    'Naive Bayes Classifier (NBC)': {\n","        'model': GaussianNB,\n","        'params': {},\n","        'param_grid': {\n","            'var_smoothing': [1e-09, 1e-08, 1e-07]\n","        }\n","    },\n","    'K-Nearest Neighbors (KNN)': {\n","        'model': KNeighborsClassifier,\n","        'params': {'n_neighbors': 5},\n","        'param_grid': {\n","            'n_neighbors': [3, 5, 7, 9],\n","            'weights': ['uniform', 'distance'],\n","            'metric': ['euclidean', 'manhattan', 'minkowski']\n","        }\n","    },\n","    'Random Forest Classifier (RFC)': {\n","        'model': RandomForestClassifier,\n","        'params': {'random_state': RANDOM_SEED},\n","        'param_grid': {\n","            'n_estimators': [50, 100, 200],\n","            'max_depth': [None, 10, 20, 30],\n","            'min_samples_split': [2, 5, 10],\n","            'min_samples_leaf': [1, 2, 4]\n","        }\n","    },\n","    'Support Vector Machine (SVM)': {\n","        'model': SVC,\n","        'params': {'random_state': RANDOM_SEED},\n","        'param_grid': {\n","            'C': [0.1, 1, 10],\n","            'kernel': ['linear', 'rbf', 'poly'],\n","            'gamma': ['scale', 'auto']\n","        }\n","    },\n","    'XGBoost Classifier': {\n","        'model': XGBClassifier,\n","        'params': {'random_state': RANDOM_SEED, 'use_label_encoder': False, 'eval_metric': 'logloss'},\n","        'param_grid': {\n","            'n_estimators': [50, 100, 200],\n","            'learning_rate': [0.01, 0.1, 0.2],\n","            'max_depth': [3, 5, 7]\n","        }\n","    },\n","    'LightGBM Classifier': {\n","        'model': LGBMClassifier,\n","        'params': {'random_state': RANDOM_SEED},\n","        'param_grid': {\n","            'n_estimators': [50, 100, 200],\n","            'learning_rate': [0.01, 0.1, 0.2],\n","            'max_depth': [-1, 10, 20]\n","        }\n","    },\n","    'CatBoost Classifier': {\n","        'model': CatBoostClassifier,\n","        'params': {'random_state': RANDOM_SEED, 'verbose': 0},  # Silent mode for CatBoost\n","        'param_grid': {\n","            'iterations': [50, 100, 200],\n","            'learning_rate': [0.01, 0.1, 0.2],\n","            'depth': [3, 5, 7]\n","        }\n","    },\n","    'Stacking Classifier': {\n","        'model': StackingClassifier,\n","        'params': {\n","            'estimators': [\n","                ('rf', RandomForestClassifier(random_state=RANDOM_SEED)),\n","                ('svc', SVC(probability=True, random_state=RANDOM_SEED))\n","            ],\n","            'final_estimator': LogisticRegression(),\n","        },\n","        'param_grid': {\n","            'final_estimator__C': [0.1, 1, 10]\n","        }\n","    },\n","    # --------------------- #\n","    # Added 'MLP Classifier (MLP)'\n","    # --------------------- #\n","    'MLP Classifier (MLP)': {\n","        'model': MLPClassifier,\n","        'params': {'random_state': RANDOM_SEED},\n","        'param_grid': {\n","            'hidden_layer_sizes': [(100,), (50, 50), (100, 50, 25)],\n","            'activation': ['relu', 'tanh', 'logistic'],\n","            'solver': ['adam', 'sgd'],\n","            'alpha': [0.0001, 0.001, 0.01],\n","            'learning_rate': ['constant', 'adaptive'],\n","        }\n","    },\n","    # --------------------- #\n","    # Updated 'RuleFit Classifier (RUC)'\n","    # --------------------- #\n","    'RuleFit Classifier (RUC)': {\n","        'model': RuleFit,  # Use RuleFit directly without calibration\n","        'params': {\n","            'random_state': RANDOM_SEED,\n","            'max_rules': 2000,\n","            'memory_par': 0.01,\n","            'tree_size': 4,\n","            'lin_trim_quantile': 0.025,\n","            'lin_standardise': True,\n","            'rfmode': 'classify',\n","            # Removed 'n_jobs' and other unsupported parameters\n","        },\n","        'param_grid': {\n","            'max_rules': [1000, 2000],\n","            'tree_size': [3, 4, 5],\n","            'memory_par': [0.01, 0.05],\n","            'lin_trim_quantile': [0.025, 0.05],\n","            'lin_standardise': [True, False]\n","        }\n","    },\n","}\n","\n","# --------------------- #\n","# Define the available feature selectors and classifiers with labels\n","# --------------------- #\n","feature_selectors = [\n","    \"Chi-Square Test (CST)\",  # FSA1\n","    \"Correlation Coefficient (CC)\",  # FSA2\n","    \"Mutual Information (MI)\",  # FSA3\n","    \"Variance Threshold (VT)\",  # FSA4\n","    \"ANOVA F-test (AFT)\",  # FSA5\n","    \"Information Gain (IG)\",  # FSA6\n","    \"Univariate Feature Selection (UFS)\",  # FSA7\n","    \"Fisher Score (FS)\", # FSA8\n","    \"LASSO\"  # FSA9\n","]\n","\n","classifiers = [\n","    \"Decision Tree Classification (DTC)\",  # C1\n","    \"Logistic Regression (LR)\",  # C2\n","    \"Linear Discriminant Analysis (LDA)\",  # C3\n","    \"Naive Bayes Classifier (NBC)\",  # C4\n","    \"K-Nearest Neighbors (KNN)\",  # C5\n","    \"Random Forest Classifier (RFC)\",  # C6\n","    \"Support Vector Machine (SVM)\",  # C7\n","    \"XGBoost Classifier\",  # C8\n","    \"LightGBM Classifier\",  # C9\n","    \"CatBoost Classifier\",  # C10\n","    \"Stacking Classifier\",  # C11\n","    \"MLP Classifier (MLP)\",  # C12\n","    \"RuleFit Classifier (RUC)\" # C13\n","]\n","\n","# --------------------- #\n","# Grid Search Configuration\n","# --------------------- #\n","# Options for GRID_SEARCH_MODE: 'exhaustive' or 'randomized'\n","GRID_SEARCH_MODE = 'randomized'  # Set to 'randomized'\n","GRID_SEARCH_ITER = 5  # Number of parameter settings sampled in RandomizedSearchCV\n","\n","### END OF PARAMETERS ###\n","\n","# ----------------------- #\n","# 4. Data Reading, Shuffling, Class Selection, and Mapping\n","# ----------------------- #\n","\n","# Read the data file\n","if FILE_PATH.endswith('.xlsx') or FILE_PATH.endswith('.xls'):\n","    Org_Data = pd.read_excel(FILE_PATH, sheet_name=SHEET_NAME)\n","elif FILE_PATH.endswith('.csv'):\n","    Org_Data = pd.read_csv(FILE_PATH)\n","else:\n","    raise ValueError(\"Unsupported file format. Please provide a .xlsx, .xls, or .csv file.\")\n","\n","# Extract the Patient ID, Data, and Outcome\n","Patient_ID = Org_Data.iloc[:, 0]\n","Data = Org_Data.iloc[:, 1:-1]   # Data (from second to second-to-last column)\n","Outcome = Org_Data.iloc[:, -1]  # Outcome (last column)\n","\n","# **Diagnostics Start**\n","print(\"Unique values in Outcome before filtering and mapping:\", Outcome.unique())\n","print(\"Number of missing values in Outcome:\", Outcome.isnull().sum())\n","\n","# Filter the dataset to include only SELECTED_CLASSES\n","mask = Outcome.isin(SELECTED_CLASSES)\n","filtered_data = Org_Data[mask].copy()\n","\n","# Shuffle the filtered_data to ensure randomness before sampling\n","filtered_data = filtered_data  # Placeholder for shuffling if needed\n","\n","# Update Patient_ID, Data, and Outcome after filtering and shuffling\n","Patient_ID = filtered_data.iloc[:, 0]\n","Data = filtered_data.iloc[:, 1:-1]\n","Outcome = filtered_data.iloc[:, -1]\n","\n","print(f\"After filtering and shuffling, number of samples: {len(Outcome)}\")\n","print(\"Unique values in Outcome after filtering and mapping:\", Outcome.unique())\n","\n","# Identify unexpected values after filtering (should not be any)\n","expected_classes = list(CLASS_MAPPING.keys())\n","unexpected_values = Outcome[~Outcome.isin(expected_classes)].unique()\n","print(\"Unexpected Outcome values not in CLASS_MAPPING after filtering:\", unexpected_values)\n","# **Diagnostics End**\n","\n","### CLASS SELECTION BY PERCENTAGE ###\n","# Select a specific percentage of patients from each class\n","def select_percentage_per_class(df, class_column, selection_percent, random_state=RANDOM_SEED):\n","    selected_df = pd.DataFrame()\n","    for cls, percent in selection_percent.items():\n","        cls_df = df[df[class_column] == cls]\n","        n_samples = int(len(cls_df) * percent)\n","        if n_samples == 0 and len(cls_df) > 0:\n","            n_samples = 1  # Ensure at least one sample is selected if possible\n","        cls_selected = cls_df.sample(n=n_samples, random_state=random_state) if n_samples > 0 else pd.DataFrame()\n","        selected_df = pd.concat([selected_df, cls_selected], axis=0)\n","    return selected_df\n","\n","# Apply selection\n","selected_data = select_percentage_per_class(\n","    filtered_data,\n","    class_column=Outcome.name,\n","    selection_percent=CLASS_SELECTION_PERCENT,\n","    random_state=RANDOM_SEED\n",")\n","print(f\"After class-wise selection, number of samples: {len(selected_data)}\")\n","\n","# Update Patient_ID, Data, and Outcome after selection\n","Patient_ID = selected_data.iloc[:, 0]\n","Data = selected_data.iloc[:, 1:-1]\n","Outcome = selected_data.iloc[:, -1]\n","\n","# Verify the selection\n","print(\"Class distribution after selection:\")\n","print(Outcome.value_counts(normalize=True))\n","\n","### CLASS MAPPING ###\n","# Map original classes to new classes as per CLASS_MAPPING\n","Outcome_mapped = Outcome.map(CLASS_MAPPING)\n","\n","# Ensure no missing values after mapping\n","if Outcome_mapped.isnull().any():\n","    raise ValueError(\"Some classes in Outcome do not have a mapping.\")\n","\n","# Update Outcome variable\n","Outcome = Outcome_mapped\n","\n","# Separate numeric and non-numeric columns\n","numeric_columns = Data.select_dtypes(include=[np.number]).columns\n","non_numeric_columns = Data.select_dtypes(exclude=[np.number]).columns\n","\n","numeric_data = Data[numeric_columns]\n","non_numeric_data = Data.select_dtypes(exclude=[np.number]).columns\n","\n","# ----------------------- #\n","# 5. Preprocessing\n","# ----------------------- #\n","\n","# Stratified train-test split for external testing\n","# Also split Patient_ID\n","X_train_num, X_test_num, y_train, y_test, Patient_ID_train, Patient_ID_test = train_test_split(\n","    numeric_data,\n","    Outcome,\n","    Patient_ID,\n","    test_size=TEST_SIZE,\n","    random_state=RANDOM_SEED,\n","    stratify=Outcome,\n",")\n","\n","# Impute only numeric data\n","if IMPUTATION_STRATEGY.startswith('SimpleImputer'):\n","    strategy = IMPUTATION_STRATEGY.split('_')[1]\n","    if strategy == 'constant':\n","        imputer = SimpleImputer(strategy=strategy, fill_value=0)  # Example fill value\n","    else:\n","        imputer = SimpleImputer(strategy=strategy)\n","elif IMPUTATION_STRATEGY == 'KNNImputer':\n","    imputer = KNNImputer()\n","elif IMPUTATION_STRATEGY == 'IterativeImputer':\n","    imputer = IterativeImputer(random_state=RANDOM_SEED)\n","else:\n","    raise ValueError(f\"Unsupported imputation strategy: {IMPUTATION_STRATEGY}\")\n","\n","X_train_num_imputed = imputer.fit_transform(X_train_num)\n","X_test_num_imputed = imputer.transform(X_test_num)\n","\n","# Scale only numeric data\n","if SCALING_METHOD == 'MinMaxScaler':\n","    scaler = MinMaxScaler()\n","elif SCALING_METHOD == 'StandardScaler':\n","    scaler = StandardScaler()\n","elif SCALING_METHOD == 'RobustScaler':\n","    scaler = RobustScaler()\n","elif SCALING_METHOD == 'Normalizer':\n","    scaler = Normalizer()\n","elif SCALING_METHOD == 'MaxAbsScaler':\n","    scaler = MaxAbsScaler()\n","elif SCALING_METHOD is None:\n","    scaler = None\n","else:\n","    raise ValueError(f\"Unsupported scaling method: {SCALING_METHOD}\")\n","\n","if scaler is not None:\n","    X_train_scaled = scaler.fit_transform(X_train_num_imputed)\n","    X_test_scaled = scaler.transform(X_test_num_imputed)\n","else:\n","    X_train_scaled = X_train_num_imputed\n","    X_test_scaled = X_test_num_imputed\n","\n","# ----------------------- #\n","# 6. Feature Selection\n","# ----------------------- #\n","\n","# Define feature selector functions\n","\n","# --------------------- #1 apply_correlation_coefficient# ---------------------\n","def apply_correlation_coefficient(X_train, y_train, NOF, **kwargs):\n","    corrs = []\n","    y_train_np = y_train.ravel()\n","    for i in range(X_train.shape[1]):\n","        corr = np.corrcoef(X_train[:, i], y_train_np)[0, 1]\n","        if np.isnan(corr):\n","            corr = 0\n","        corrs.append(abs(corr))\n","    corrs = np.array(corrs)\n","    top_k_idx = np.argsort(corrs)[-NOF:]\n","    X_train_corr = X_train[:, top_k_idx]\n","    return X_train_corr, top_k_idx\n","\n","# --------------------- #2 apply_chi_square# ---------------------\n","def apply_chi_square(X_train, y_train, NOF, **kwargs):\n","    score_func = kwargs.get('score_func', chi2)\n","    chi2_selector = SelectKBest(score_func=score_func, k=NOF)\n","    X_train_chi2 = chi2_selector.fit_transform(X_train, y_train)\n","    return X_train_chi2, chi2_selector\n","\n","# --------------------- #3 apply_mutual_information# ---------------------\n","def apply_mutual_information(X_train, y_train, NOF, **kwargs):\n","    mi_selector = SelectKBest(score_func=mutual_info_classif, k=NOF)\n","    X_train_mi = mi_selector.fit_transform(X_train, y_train)\n","    return X_train_mi, mi_selector\n","\n","# --------------------- #4 apply_variance_threshold# ---------------------\n","def apply_variance_threshold(X_train, y_train, NOF, **kwargs):\n","    # VarianceThreshold removes all features whose variance doesn't meet the threshold.\n","    # Here, we compute the threshold to select top NOF features based on variance.\n","    variances = np.var(X_train, axis=0)\n","    top_k_idx = np.argsort(variances)[-NOF:]\n","    X_train_var = X_train[:, top_k_idx]\n","    return X_train_var, top_k_idx\n","\n","# --------------------- #5 apply_anova_f_test# ---------------------\n","def apply_anova_f_test(X_train, y_train, NOF, **kwargs):\n","    anova_selector = SelectKBest(score_func=f_classif, k=NOF)\n","    X_train_anova = anova_selector.fit_transform(X_train, y_train)\n","    return X_train_anova, anova_selector\n","\n","# --------------------- #6 apply_information_gain# ---------------------\n","def apply_information_gain(X_train, y_train, NOF, **kwargs):\n","    # Information Gain is equivalent to Mutual Information for classification tasks\n","    ig_selector = SelectKBest(score_func=mutual_info_classif, k=NOF)\n","    X_train_ig = ig_selector.fit_transform(X_train, y_train)\n","    return X_train_ig, ig_selector\n","\n","# --------------------- #7 apply_univariate_feature_selection# ---------------------\n","def apply_univariate_feature_selection(X_train, y_train, NOF, **kwargs):\n","    score_func = kwargs.get('score_func', mutual_info_classif)\n","    ufs_selector = SelectKBest(score_func=score_func, k=NOF)\n","    X_train_ufs = ufs_selector.fit_transform(X_train, y_train)\n","    return X_train_ufs, ufs_selector\n","\n","# --------------------- #8 apply_fisher_score# ---------------------\n","def apply_fisher_score(X_train, y_train, NOF, **kwargs):\n","    \"\"\"\n","    Manual implementation of Fisher Score for feature selection.\n","    \"\"\"\n","    classes = np.unique(y_train)\n","    mean_total = np.mean(X_train, axis=0)\n","    numerator = 0\n","    denominator = 0\n","    for cls in classes:\n","        X_cls = X_train[y_train == cls]\n","        n_cls = X_cls.shape[0]\n","        mean_cls = np.mean(X_cls, axis=0)\n","        var_cls = np.var(X_cls, axis=0)\n","        numerator += n_cls * (mean_cls - mean_total) ** 2\n","        denominator += n_cls * var_cls\n","    # To avoid division by zero\n","    denominator = np.where(denominator == 0, 1e-10, denominator)\n","    fisher_scores = numerator / denominator\n","    top_k_idx = np.argsort(fisher_scores)[-NOF:]\n","    X_train_fs = X_train[:, top_k_idx]\n","    return X_train_fs, top_k_idx\n","\n","# --------------------- #9 apply_lasso# ---------------------\n","def apply_lasso(X_train, y_train, NOF, **kwargs):\n","    alpha = kwargs.get('alpha', 0.01)\n","    lasso = Lasso(alpha=alpha, random_state=RANDOM_SEED)\n","    lasso.fit(X_train, y_train)\n","\n","    # Get the indices of features with non-zero coefficients\n","    selected_idx = np.where(lasso.coef_ != 0)[0]\n","\n","    # Check if the number of selected features is less than NOF\n","    if len(selected_idx) < NOF:\n","        # If fewer features are selected, select additional top features based on absolute coefficient values\n","        coef_abs_sorted = np.argsort(np.abs(lasso.coef_))[-NOF:]  # Sort by absolute value of coefficients\n","        selected_idx = np.union1d(selected_idx, coef_abs_sorted)  # Combine selected and additional features\n","    else:\n","        # Sort the selected features by absolute coefficient value and select the top NOF\n","        coef_abs_selected = np.abs(lasso.coef_[selected_idx])\n","        sorted_order = np.argsort(coef_abs_selected)[::-1]\n","        selected_idx = selected_idx[sorted_order][:NOF]\n","\n","    # Ensure selected_idx is a NumPy array before indexing\n","    selected_idx = np.array(selected_idx)\n","\n","    # Select the top features from the training data\n","    X_train_selected = X_train[:, selected_idx]\n","\n","    return X_train_selected, selected_idx\n","\n","# Map function names to actual functions\n","feature_selector_functions = {\n","    'apply_chi_square': apply_chi_square,\n","    'apply_correlation_coefficient': apply_correlation_coefficient,\n","    'apply_mutual_information': apply_mutual_information,\n","    'apply_variance_threshold': apply_variance_threshold,\n","    'apply_anova_f_test': apply_anova_f_test,\n","    'apply_information_gain': apply_information_gain,\n","    'apply_univariate_feature_selection': apply_univariate_feature_selection,\n","    'apply_fisher_score': apply_fisher_score,\n","    'apply_lasso': apply_lasso,\n","}\n","\n","# ----------------------- #\n","# 7. Applying Feature Selection and Classifiers\n","# ----------------------- #\n","\n","# To store the results\n","results = []\n","selected_features_all = []\n","confusion_matrices_all = []\n","best_parameters_all = []  # To store best hyperparameters\n","\n","# Create results directory with timestamp\n","current_time = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n","results_directory = os.path.join(BASE_RESULTS_DIRECTORY, f\"results_{current_time}\")\n","os.makedirs(results_directory, exist_ok=True)\n","\n","# Create Predicted_Outcome directory and subdirectories\n","predicted_outcome_directory = os.path.join(results_directory, 'Predicted_Outcome')\n","fivefold_cv_directory = os.path.join(predicted_outcome_directory, 'Fivefold Cross Validation')\n","external_test_directory = os.path.join(predicted_outcome_directory, 'External Testing')\n","tuning_hyperparameters_directory = os.path.join(results_directory, 'Tuning_Hyperparameters')\n","\n","os.makedirs(fivefold_cv_directory, exist_ok=True)\n","os.makedirs(external_test_directory, exist_ok=True)\n","os.makedirs(tuning_hyperparameters_directory, exist_ok=True)\n","\n","# Initialize StratifiedKFold\n","skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_SEED)\n","\n","# Modify the loop to iterate through the 'feature_selectors' list\n","for feature_selector_name in feature_selectors:\n","    if feature_selector_name not in INVOLVED_FEATURE_SELECTORS:\n","        print(f\"Feature Selector '{feature_selector_name}' not found in INVOLVED_FEATURE_SELECTORS. Skipping.\")\n","        continue  # Skip if not defined\n","\n","    fs_info = INVOLVED_FEATURE_SELECTORS[feature_selector_name]\n","    print(f\"\\n\\033[1m--- Applying Feature Selector: {feature_selector_name} ---\\033[0m\")\n","\n","    fs_function_name = fs_info['function']\n","    fs_params = fs_info['params']\n","    feature_selector_function = feature_selector_functions.get(fs_function_name)\n","\n","    if feature_selector_function is None:\n","        print(f\"Feature selector function '{fs_function_name}' not found. Skipping '{feature_selector_name}'.\")\n","        continue  # Skip if function not defined\n","\n","    X_train_selected, selector_obj = feature_selector_function(X_train_scaled, y_train, NOF, **fs_params)\n","\n","    # Get the feature names\n","    if isinstance(selector_obj, SelectKBest):\n","        selected_feature_names = numeric_columns[selector_obj.get_support()].tolist()\n","    elif isinstance(selector_obj, np.ndarray):\n","        # Assume selector_obj contains integer indices\n","        selected_feature_names = numeric_columns.take(selector_obj).tolist()\n","    elif isinstance(selector_obj, list):\n","        selected_feature_names = numeric_columns.take(selector_obj).tolist()\n","    else:\n","        # For selectors that return indices\n","        selected_feature_names = numeric_columns.take(selector_obj).tolist()\n","\n","    # Convert to list of strings if necessary\n","    selected_feature_names = [str(feature) for feature in selected_feature_names]\n","\n","    print(f\"Selected Features: {selected_feature_names}\")\n","\n","    # Check for duplicates\n","    if len(selected_feature_names) != len(set(selected_feature_names)):\n","        print(\"Warning: Duplicate features found in selected features.\")\n","        selected_feature_names = list(dict.fromkeys(selected_feature_names))  # Remove duplicates\n","        print(f\"Duplicate features have been removed. Updated Selected Features: {selected_feature_names}\")\n","\n","    selected_features_all.append({\n","        'Feature Selector': feature_selector_name,\n","        'Selected Features': selected_feature_names,\n","    })\n","\n","    # Apply the same feature selection to the test set\n","    if hasattr(selector_obj, 'transform'):\n","        X_test_selected = selector_obj.transform(X_test_scaled)\n","    elif isinstance(selector_obj, (np.ndarray, list)):\n","        X_test_selected = X_test_scaled[:, selector_obj]\n","    else:\n","        # For selectors that return indices\n","        X_test_selected = X_test_scaled[:, selector_obj]\n","\n","    # --------------------- #\n","    # 7.1. Iterate Through Classifiers\n","    # --------------------- #\n","\n","    # To store best parameters for each classifier\n","    best_parameters = {}\n","\n","    for clf_label in classifiers:\n","        clf_name = clf_label  # Since classifiers list contains the full name\n","        if clf_name not in INVOLVED_CLASSIFIERS:\n","            print(f\"Classifier '{clf_name}' not found in INVOLVED_CLASSIFIERS. Skipping.\")\n","            continue  # Skip classifiers not defined\n","\n","        clf_info = INVOLVED_CLASSIFIERS[clf_name]\n","        print(f\"\\n\\033[1m--- Performing Grid Search for {clf_name} ---\\033[0m\")\n","\n","        model_class = clf_info['model']\n","        model_initial_params = clf_info['params'].copy()  # Use a copy to prevent mutation\n","        param_grid = clf_info.get('param_grid', {})\n","\n","        # Instantiate the classifier with initial parameters\n","        try:\n","            model = model_class(**model_initial_params)\n","        except TypeError as e:\n","            print(f\"Error initializing model for {clf_name}: {e}\")\n","            continue  # Skip this classifier\n","\n","        # Initialize GridSearchCV or RandomizedSearchCV based on GRID_SEARCH_MODE\n","        if GRID_SEARCH_MODE == 'exhaustive':\n","            search = GridSearchCV(\n","                estimator=model,\n","                param_grid=param_grid,\n","                cv=3,  # 3-fold cross-validation for grid search\n","                scoring='accuracy',  # You can change this to other metrics if desired\n","                n_jobs=-1,\n","                verbose=1\n","            )\n","        elif GRID_SEARCH_MODE == 'randomized':\n","            # Ensure param_grid is a dictionary\n","            if isinstance(param_grid, dict):\n","                param_distributions = param_grid\n","            elif isinstance(param_grid, list):\n","                # If param_grid is a list of dicts, RandomizedSearchCV cannot handle it directly\n","                # Therefore, we can use GridSearchCV in this case\n","                print(f\"Param grid for {clf_name} is a list. Switching to GridSearchCV.\")\n","                search = GridSearchCV(\n","                    estimator=model,\n","                    param_grid=param_grid,\n","                    cv=3,\n","                    scoring='accuracy',\n","                    n_jobs=-1,\n","                    verbose=1\n","                )\n","                try:\n","                    search.fit(X_train_selected, y_train)\n","                    best_params = search.best_params_\n","                    best_score = search.best_score_\n","                    print(f\"Best parameters for {clf_name}: {best_params}\")\n","                    print(f\"Best cross-validation accuracy: {best_score:.4f}\")\n","                    best_parameters[clf_name] = best_params\n","\n","                    # Save the best parameters to CSV\n","                    best_params_df = pd.DataFrame([best_params])\n","                    tuning_save_path = os.path.join(\n","                        tuning_hyperparameters_directory,\n","                        f'best_parameters_{feature_selector_name}_{clf_name}.csv'\n","                    )\n","                    best_params_df.to_csv(tuning_save_path, index=False)\n","                    print(f\"Best parameters saved to: {tuning_save_path}\")\n","\n","                except Exception as e:\n","                    print(f\"Grid search failed for {clf_name} with error: {e}\")\n","                    best_parameters[clf_name] = model_initial_params  # Fallback to initial params\n","                continue  # Move to the next classifier\n","\n","            search = RandomizedSearchCV(\n","                estimator=model,\n","                param_distributions=param_distributions,\n","                n_iter=GRID_SEARCH_ITER,  # Now set to 5\n","                cv=3,  # 3-fold cross-validation for randomized search\n","                scoring='accuracy',  # You can change this to other metrics if desired\n","                random_state=RANDOM_SEED,\n","                n_jobs=-1,\n","                verbose=1\n","            )\n","        else:\n","            raise ValueError(\"Invalid GRID_SEARCH_MODE. Choose 'exhaustive' or 'randomized'.\")\n","\n","        # Fit GridSearchCV or RandomizedSearchCV on the entire selected training data\n","        try:\n","            search.fit(X_train_selected, y_train)\n","            best_params = search.best_params_\n","            best_score = search.best_score_\n","            print(f\"Best parameters for {clf_name}: {best_params}\")\n","            print(f\"Best cross-validation accuracy: {best_score:.4f}\")\n","            best_parameters[clf_name] = best_params\n","\n","            # Save the best parameters to CSV\n","            best_params_df = pd.DataFrame([best_params])\n","            tuning_save_path = os.path.join(\n","                tuning_hyperparameters_directory,\n","                f'best_parameters_{feature_selector_name}_{clf_name}.csv'\n","            )\n","            best_params_df.to_csv(tuning_save_path, index=False)\n","            print(f\"Best parameters saved to: {tuning_save_path}\")\n","\n","        except Exception as e:\n","            print(f\"Grid search failed for {clf_name} with error: {e}\")\n","            best_parameters[clf_name] = model_initial_params  # Fallback to initial params\n","\n","    # Save all best parameters for the current feature selector\n","    best_parameters_all.append({\n","        'Feature Selector': feature_selector_name,\n","        'Best Parameters': best_parameters\n","    })\n","\n","    # --------------------- #\n","    # 7.2. K-Fold Cross-Validation with Tuned Hyperparameters\n","    # --------------------- #\n","\n","    for clf_label in classifiers:\n","        clf_name = clf_label\n","        if clf_name not in INVOLVED_CLASSIFIERS:\n","            print(f\"Classifier '{clf_name}' not found in INVOLVED_CLASSIFIERS. Skipping.\")\n","            continue  # Skip classifiers not defined\n","\n","        clf_info = INVOLVED_CLASSIFIERS[clf_name]\n","        print(f\"\\n\\033[1m--- {N_FOLDS}-Fold Cross-Validation with {clf_name} ---\\033[0m\")\n","\n","        model_class = clf_info['model']\n","        model_initial_params = clf_info['params'].copy()\n","        best_params = best_parameters.get(clf_name, model_initial_params).copy()\n","\n","        # Instantiate the model with best hyperparameters\n","        try:\n","            model = model_class(**{**model_initial_params, **best_params})\n","        except TypeError as e:\n","            print(f\"Error initializing {clf_name} with parameters {best_params}: {e}\")\n","            print(\"Falling back to initial parameters.\")\n","            try:\n","                model = model_class(**model_initial_params)\n","            except Exception as e_inner:\n","                print(f\"Failed to initialize {clf_name} with initial parameters: {e_inner}\")\n","                continue  # Skip this classifier\n","\n","        # Initialize metrics\n","        fold_metrics = {\n","            'fold': [], 'validation_accuracy': [], 'validation_precision': [], 'validation_recall': [],\n","            'validation_f1_score': [], 'test_accuracy': [], 'test_precision': [],\n","            'test_recall': [], 'test_f1_score': [],\n","        }\n","        confusion_matrices = []\n","\n","        # Initialize lists to store predictions\n","        cv_true_labels = []\n","        cv_predicted_labels = []\n","        cv_patient_ids = []\n","        cv_fold_numbers = []\n","\n","        test_true_labels = []\n","        test_predicted_labels = []\n","        test_patient_ids = []\n","        test_fold_numbers = []\n","\n","        validation_results = []\n","        test_results = []\n","\n","        # Loop through the folds of cross-validation\n","        for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_selected, y_train), 1):\n","            # Instantiate a fresh model for each fold to prevent estimator accumulation\n","            try:\n","                model_fold = model_class(**{**model_initial_params, **best_params})\n","            except TypeError as e:\n","                print(f\"Error initializing {clf_name} for fold {fold} with parameters {best_params}: {e}\")\n","                print(\"Falling back to initial parameters.\")\n","                try:\n","                    model_fold = model_class(**model_initial_params)\n","                except Exception as e_inner:\n","                    print(f\"Failed to initialize {clf_name} for fold {fold}: {e_inner}\")\n","                    continue  # Skip this fold\n","\n","            X_train_fold, X_val_fold = X_train_selected[train_idx], X_train_selected[val_idx]\n","            y_train_fold, y_val_fold = y_train.values[train_idx], y_train.values[val_idx]\n","            Patient_ID_train_fold = Patient_ID_train.iloc[train_idx]\n","            Patient_ID_val_fold = Patient_ID_train.iloc[val_idx]\n","\n","            # Fit the model on the current fold\n","            try:\n","                model_fold.fit(X_train_fold, y_train_fold)\n","            except Exception as e:\n","                print(f\"Error fitting model {clf_name} on fold {fold}: {e}\")\n","                continue\n","\n","            y_val_pred = model_fold.predict(X_val_fold)\n","\n","            # Append validation predictions and true labels along with Patient IDs\n","            cv_true_labels.extend(y_val_fold)\n","            cv_predicted_labels.extend(y_val_pred)\n","            cv_patient_ids.extend(Patient_ID_val_fold)\n","            cv_fold_numbers.extend([fold] * len(y_val_fold))\n","\n","            # Validation metrics\n","            val_accuracy = accuracy_score(y_val_fold, y_val_pred)\n","            val_precision = precision_score(y_val_fold, y_val_pred, average='weighted', zero_division=0)\n","            val_recall = recall_score(y_val_fold, y_val_pred, average='weighted', zero_division=0)\n","            val_f1 = f1_score(y_val_fold, y_val_pred, average='weighted', zero_division=0)\n","\n","            validation_results.append({\n","                'Fold': fold, 'Validation Accuracy': val_accuracy, 'Validation Precision': val_precision,\n","                'Validation Recall': val_recall, 'Validation F1-Score': val_f1,\n","            })\n","\n","            # Test set predictions\n","            y_test_pred = model_fold.predict(X_test_selected)\n","\n","            # Append test predictions and true labels along with Patient IDs\n","            test_true_labels.extend(y_test)\n","            test_predicted_labels.extend(y_test_pred)\n","            test_patient_ids.extend(Patient_ID_test)\n","            test_fold_numbers.extend([fold] * len(y_test))  # Use the same fold number for consistency\n","\n","            test_accuracy = accuracy_score(y_test, y_test_pred)\n","            test_precision = precision_score(y_test, y_test_pred, average='weighted', zero_division=0)\n","            test_recall = recall_score(y_test, y_test_pred, average='weighted', zero_division=0)\n","            test_f1 = f1_score(y_test, y_test_pred, average='weighted', zero_division=0)\n","\n","            test_results.append({\n","                'Fold': fold, 'Test Accuracy': test_accuracy, 'Test Precision': test_precision,\n","                'Test Recall': test_recall, 'Test F1-Score': test_f1,\n","            })\n","\n","            fold_metrics['fold'].append(fold)\n","            fold_metrics['validation_accuracy'].append(val_accuracy)\n","            fold_metrics['validation_precision'].append(val_precision)\n","            fold_metrics['validation_recall'].append(val_recall)\n","            fold_metrics['validation_f1_score'].append(val_f1)\n","            fold_metrics['test_accuracy'].append(test_accuracy)\n","            fold_metrics['test_precision'].append(test_precision)\n","            fold_metrics['test_recall'].append(test_recall)\n","            fold_metrics['test_f1_score'].append(test_f1)\n","\n","            confusion_val = confusion_matrix(y_val_fold, y_val_pred)\n","            confusion_matrices.append({\n","                'Feature Selector': feature_selector_name,\n","                'Classifier': clf_name,\n","                'Fold': fold,\n","                'Type': 'Validation',\n","                'Confusion Matrix': confusion_val.tolist(),\n","            })\n","\n","            confusion_test = confusion_matrix(y_test, y_test_pred)\n","            confusion_matrices.append({\n","                'Feature Selector': feature_selector_name,\n","                'Classifier': clf_name,\n","                'Fold': fold,\n","                'Type': 'External Test',\n","                'Confusion Matrix': confusion_test.tolist(),\n","            })\n","\n","        print(f\"\\n\\033[1mResults of {N_FOLDS} Validation Folds:\\033[0m\")\n","        for res in validation_results:\n","            print(f\"Fold {res['Fold']} - Validation Accuracy: {res['Validation Accuracy']:.4f}, \"\n","                  f\"Precision: {res['Validation Precision']:.4f}, Recall: {res['Validation Recall']:.4f}, \"\n","                  f\"F1-Score: {res['Validation F1-Score']:.4f}\")\n","\n","        val_acc_mean = np.mean(fold_metrics['validation_accuracy'])\n","        val_acc_std = np.std(fold_metrics['validation_accuracy'])\n","        val_prec_mean = np.mean(fold_metrics['validation_precision'])\n","        val_prec_std = np.std(fold_metrics['validation_precision'])\n","        val_rec_mean = np.mean(fold_metrics['validation_recall'])\n","        val_rec_std = np.std(fold_metrics['validation_recall'])\n","        val_f1_mean = np.mean(fold_metrics['validation_f1_score'])\n","        val_f1_std = np.std(fold_metrics['validation_f1_score'])\n","\n","        print(f\"\\n\\033[1mAverage Validation Metrics:\\033[0m\")\n","        print(f\"Accuracy: {val_acc_mean:.4f} ± {val_acc_std:.4f}, \"\n","              f\"Precision: {val_prec_mean:.4f} ± {val_prec_std:.4f}, \"\n","              f\"Recall: {val_rec_mean:.4f} ± {val_rec_std:.4f}, \"\n","              f\"F1-Score: {val_f1_mean:.4f} ± {val_f1_std:.4f}\")\n","\n","        print(f\"\\n\\033[1mResults of {N_FOLDS} External Testing:\\033[0m\")\n","        for res in test_results:\n","            print(f\"Fold {res['Fold']} - Test Accuracy: {res['Test Accuracy']:.4f}, \"\n","                  f\"Precision: {res['Test Precision']:.4f}, Recall: {res['Test Recall']:.4f}, \"\n","                  f\"F1-Score: {res['Test F1-Score']:.4f}\")\n","\n","        test_acc_mean = np.mean(fold_metrics['test_accuracy'])\n","        test_acc_std = np.std(fold_metrics['test_accuracy'])\n","        test_prec_mean = np.mean(fold_metrics['test_precision'])\n","        test_prec_std = np.std(fold_metrics['test_precision'])\n","        test_rec_mean = np.mean(fold_metrics['test_recall'])\n","        test_rec_std = np.std(fold_metrics['test_recall'])\n","        test_f1_mean = np.mean(fold_metrics['test_f1_score'])\n","        test_f1_std = np.std(fold_metrics['test_f1_score'])\n","\n","        print(f\"\\n\\033[1mAverage External Test Metrics:\\033[0m\")\n","        print(f\"Accuracy: {test_acc_mean:.4f} ± {test_acc_std:.4f}, \"\n","              f\"Precision: {test_prec_mean:.4f} ± {test_prec_std:.4f}, \"\n","              f\"Recall: {test_rec_mean:.4f} ± {test_rec_std:.4f}, \"\n","              f\"F1-Score: {test_f1_mean:.4f} ± {test_f1_std:.4f}\")\n","\n","        fold_metrics['Feature Selector'] = [feature_selector_name] * N_FOLDS\n","        fold_metrics['Classifier'] = [clf_name] * N_FOLDS\n","\n","        # Check if all lists in fold_metrics have the same length\n","        lengths = [len(v) for v in fold_metrics.values()]\n","        if len(set(lengths)) != 1:\n","            print(\"Warning: Not all metric lists have the same length. Skipping saving this fold's metrics.\")\n","            continue  # Skip if lengths are inconsistent\n","\n","        df_fold_metrics = pd.DataFrame(fold_metrics)\n","        results.append(df_fold_metrics)\n","\n","        confusion_matrices_all.extend(confusion_matrices)\n","\n","        # Save cross-validation predictions\n","        cv_results_df = pd.DataFrame({\n","            'Patient ID': cv_patient_ids,\n","            'Fold': cv_fold_numbers,\n","            'True Label': cv_true_labels,\n","            'Predicted Label': cv_predicted_labels\n","        })\n","\n","        # Create subdirectory path\n","        cv_subdirectory = os.path.join(\n","            fivefold_cv_directory,\n","            f\"{feature_selector_name}_{clf_name}\"\n","        )\n","        os.makedirs(cv_subdirectory, exist_ok=True)\n","\n","        cv_predictions_save_path = os.path.join(\n","            cv_subdirectory,\n","            f'cv_predictions_{feature_selector_name}_{clf_name}.csv'\n","        )\n","\n","        cv_results_df.to_csv(cv_predictions_save_path, index=False)\n","\n","        # Save external test predictions\n","        test_results_df = pd.DataFrame({\n","            'Patient ID': test_patient_ids,\n","            'Fold': test_fold_numbers,\n","            'True Label': test_true_labels,\n","            'Predicted Label': test_predicted_labels\n","        })\n","\n","        # Create subdirectory path\n","        test_subdirectory = os.path.join(\n","            external_test_directory,\n","            f\"{feature_selector_name}_{clf_name}\"\n","        )\n","        os.makedirs(test_subdirectory, exist_ok=True)\n","\n","        test_predictions_save_path = os.path.join(\n","            test_subdirectory,\n","            f'test_predictions_{feature_selector_name}_{clf_name}.csv'\n","        )\n","\n","        test_results_df.to_csv(test_predictions_save_path, index=False)\n","\n","    # Save selected features\n","    selected_features_df = pd.DataFrame(selected_features_all)\n","\n","    # Expand the 'Selected Features' column into separate columns for clarity\n","    selected_features_expanded = selected_features_df['Selected Features'].apply(pd.Series)\n","    selected_features_expanded.columns = [f'Feature_{i+1}' for i in selected_features_expanded.columns]\n","\n","    # Combine the feature selector names with the expanded features\n","    selected_features_combined = pd.concat([selected_features_df[['Feature Selector']], selected_features_expanded], axis=1)\n","\n","    # Save to CSV\n","    selected_features_save_path = os.path.join(results_directory, 'selected_features.csv')\n","    selected_features_combined.to_csv(selected_features_save_path, index=False)\n","    print(f\"Selected Features have been saved to: {selected_features_save_path}\")\n","\n","    # Save best parameters\n","    # Convert the list of dictionaries to a DataFrame\n","    best_parameters_records = []\n","    for record in best_parameters_all:\n","        feature_selector = record['Feature Selector']\n","        for clf, params in record['Best Parameters'].items():\n","            # Prefix classifier name to parameter keys to avoid duplicates\n","            params_flat = {f\"{clf}_{k}\": v for k, v in params.items()}\n","            params_flat['Feature Selector'] = feature_selector\n","            params_flat['Classifier'] = clf\n","            best_parameters_records.append(params_flat)\n","\n","    best_parameters_df = pd.DataFrame(best_parameters_records)\n","    best_parameters_save_path = os.path.join(tuning_hyperparameters_directory, 'best_parameters.csv')\n","    best_parameters_df.to_csv(best_parameters_save_path, index=False)\n","    print(f\"Best Hyperparameters have been saved to: {best_parameters_save_path}\")\n","\n","    # ----------------------- #\n","    # 8. Saving and Aggregating Results\n","    # ----------------------- #\n","\n","    if results:\n","        results_df = pd.concat(results, ignore_index=True)\n","        average_metrics = results_df.groupby(['Feature Selector', 'Classifier']).agg({\n","            'validation_accuracy': ['mean', 'std'],\n","            'validation_precision': ['mean', 'std'],\n","            'validation_recall': ['mean', 'std'],\n","            'validation_f1_score': ['mean', 'std'],\n","            'test_accuracy': ['mean', 'std'],\n","            'test_precision': ['mean', 'std'],\n","            'test_recall': ['mean', 'std'],\n","            'test_f1_score': ['mean', 'std'],\n","        }).reset_index()\n","\n","        average_metrics.columns = [' '.join(col).strip() if col[1] else col[0] for col in average_metrics.columns.values]\n","\n","        # Define save paths\n","        results_save_path = os.path.join(results_directory, 'evaluation_metrics.csv')\n","        average_metrics_save_path = os.path.join(results_directory, 'average_metrics.csv')\n","        confusion_matrix_save_path = os.path.join(results_directory, 'confusion_matrices.csv')\n","        workflow_save_path = os.path.join(results_directory, 'Workflow_Ver18.txt')\n","        code_save_path = os.path.join(results_directory, 'Code_Ver18.py')  # Updated to Ver18\n","\n","        # Save metrics\n","        results_df.to_csv(results_save_path, index=False)\n","        average_metrics.to_csv(average_metrics_save_path, index=False)\n","        print(f\"Evaluation Metrics have been saved to: {results_save_path}\")\n","        print(f\"Average Metrics have been saved to: {average_metrics_save_path}\")\n","\n","        # Save standard deviations separately\n","        # Extract std columns\n","        std_columns = [col for col in average_metrics.columns if 'std' in col]\n","        std_metrics_df = average_metrics[['Feature Selector', 'Classifier'] + std_columns]\n","        std_metrics_save_path = os.path.join(results_directory, 'STD_metrics.csv')\n","        std_metrics_df.to_csv(std_metrics_save_path, index=False)\n","        print(f\"Standard Deviations of evaluation metrics have been saved to: {std_metrics_save_path}\")\n","\n","        # Save confusion matrices\n","        confusion_matrices_df = pd.DataFrame(confusion_matrices_all)\n","        confusion_matrices_df.to_csv(confusion_matrix_save_path, index=False)\n","        print(f\"Confusion Matrices have been saved to: {confusion_matrix_save_path}\")\n","\n","        # Copy original data file to results directory\n","        original_data_filename = os.path.basename(FILE_PATH)\n","        original_data_filename_with_label = f\"Original_{original_data_filename}\"\n","        destination_path = os.path.join(results_directory, original_data_filename_with_label)\n","        shutil.copyfile(FILE_PATH, destination_path)\n","        print(f\"Original data file has been copied to: {destination_path}\")\n","\n","        # Save workflow description (placeholder)\n","        workflow_text = \"\"\"\n","    [Workflow_Ver18 content goes here]\n","    \"\"\"\n","        with open(workflow_save_path, 'w') as workflow_file:\n","            workflow_file.write(workflow_text)\n","        print(f\"Workflow has been saved to: {workflow_save_path}\")\n","\n","        # Save the current script's code\n","        try:\n","            # Since we've modified the code structure, ensure that the current script can be retrieved\n","            code_text = inspect.getsource(inspect.currentframe())\n","            with open(code_save_path, 'w') as code_file:\n","                code_file.write(code_text)\n","            print(f\"Code has been saved to: {code_save_path}\")\n","        except Exception as e:\n","            print(f\"Unable to save the code: {e}\")\n","\n","    else:\n","        print(\"No results to save. Please check your data and parameters.\")\n","\n","# ======================= #\n","#       End of Script or Classification Codes    #\n","# ======================= #\n"],"metadata":{"id":"Z1yA41bUCkat"},"id":"Z1yA41bUCkat","execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.5"}},"nbformat":4,"nbformat_minor":5}